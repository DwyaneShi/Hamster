#!/bin/bash
#############################################################################
#  Copyright (C) 2016
#  
#  Hamster is free software; you can redistribute it and/or modify it
#  under the terms of the GNU General Public License as published by
#  the Free Software Foundation; either version 2 of the License, or
#  (at your option) any later version.
#  
#  Hamster is distributed in the hope that it will be useful, but
#  WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
#  General Public License for more details.
#  
#  You should have received a copy of the GNU General Public License
#  along with Hamster.  If not, see <http://www.gnu.org/licenses/>.
#############################################################################

# This script sets up configuration files for jobs.  For the most
# part, it shouldn't be edited.  See job submission files for
# configuration details.
source ${HAMSTER_SCRIPTS_HOME}/util/hamster-io

if [ "${HADOOP_SETUP}" != "yes" ]
then
    exit 0
else
    Hamster_echo "Setting up Hadoop"
fi

source ${HAMSTER_SCRIPTS_HOME}/lib/exports/hamster-exports-submission-type
source ${HAMSTER_SCRIPTS_HOME}/lib/exports/hamster-exports-dirs
source ${HAMSTER_SCRIPTS_HOME}/lib/exports/hamster-exports-user
source ${HAMSTER_SCRIPTS_HOME}/lib/hamster-lib-defaults
source ${HAMSTER_SCRIPTS_HOME}/lib/hamster-lib-node-identification
source ${HAMSTER_SCRIPTS_HOME}/lib/hamster-lib-hadoop-helper
source ${HAMSTER_SCRIPTS_HOME}/lib/hamster-lib-setup
source ${HAMSTER_SCRIPTS_HOME}/lib/hamster-lib-helper
source ${HAMSTER_SCRIPTS_HOME}/lib/hamster-lib-log

# hadoopnoderank set if succeed
if ! Hamster_am_I_a_hadoop_node
then
    Hamster_echo "I am not a Hadoop node"
    exit 0
fi

# For rest of setup, we will use cluster specific paths
Hamster_make_all_local_dirs_node_specific

extrahadoopclasses=""
extrahadoopopts=""
extrayarnopts=""
extrahadooptaskopts=""

if [ -f ${HAMSTER_SCRIPTS_HOME}/benchmarks/extra_cp.sh ]
then
    extrahadoopclasses=${extrahadoopclasses}:$(${HAMSTER_SCRIPTS_HOME}/benchmarks/extra_cp.sh)
fi

slave_file=$(Hamster_hadoop_slave_or_worker_file)
cp ${HADOOP_CONF_DIR}/${slave_file} ${HADOOP_CONF_DIR}/hosts-include
hostsincludefile="${HADOOP_CONF_DIR}/hosts-include"
hostsincludefilesubst=`echo "${hostsincludefile}" | sed "s/\\//\\\\\\\\\//g"`

# By default leave exclude file empty
touch ${HADOOP_CONF_DIR}/hosts-exclude
hostsexcludefile="${HADOOP_CONF_DIR}/hosts-exclude"
hostsexcludefilesubst=`echo "${hostsexcludefile}" | sed "s/\\//\\\\\\\\\//g"`

#
# Calculate values for various config file variables, based on
# recommendtions, rules of thumb, or based on what user input.
#

# Recommendation from Cloudera, parallel copies sqrt(number of nodes), floor of ten
if [ "${HADOOP_PARALLEL_COPIES}X" != "X" ]
then
    parallelcopies=${HADOOP_PARALLEL_COPIES}
else 
    parallelcopies=`echo "sqrt ( ${HADOOP_SLAVE_COUNT} )" | bc -l | xargs printf "%1.0f"`
    if [ "${parallelcopies}" -lt "10" ]
    then
        parallelcopies=10
    fi
fi
Hamster_print HADOOP_PARALLEL_COPIES

# Recommendation from Cloudera, 10% of nodes w/ floor of ten, ceiling 200
# My experience this is low b/c of high core counts, so bump higher to 50% 
namenodehandlercount=`echo "${HADOOP_SLAVE_COUNT} * .5" | bc -l | xargs printf "%1.0f"`
if [ "${namenodehandlercount}" -lt "10" ]
then
    namenodehandlercount=10
fi

if [ "${namenodehandlercount}" -gt "200" ]
then
    namenodehandlercount=200
fi
Hamster_print namenodehandlercount "Namenode Handler Count"

# General rule of thumb is half namenode handler count, so * .25 instead of * .5
datanodehandlercount=`echo "${HADOOP_SLAVE_COUNT} * .25" | bc -l | xargs printf "%1.0f"`
if [ "${datanodehandlercount}" -lt "10" ]
then
    datanodehandlercount=10
fi

if [ "${datanodehandlercount}" -gt "200" ]
then
    datanodehandlercount=200
fi
Hamster_print datanodehandlercount "Datanode Handler Count"

# Per description, about 4% of nodes but w/ floor of 10
jobtrackerhandlercount=`echo "${HADOOP_SLAVE_COUNT} * .04" | bc -l | xargs printf "%1.0f"`
if [ "${jobtrackerhandlercount}" -lt "10" ]
then
    jobtrackerhandlercount=10
fi
Hamster_print jobtrackerhandlercount "Job Tracker Handler Count"

# Per descrption, about square root number of nodes
submitfilereplication=`echo "sqrt ( ${HADOOP_SLAVE_COUNT} )" | bc -l | xargs printf "%1.0f"`
Hamster_print submitfilereplication "Submit File Replication"

# Optimal depends on file system
if [ "${HADOOP_FILESYSTEM_MODE}" == "hdfs" ]
then
    iobuffersize=65536
elif Hamster_hadoop_filesystem_mode_is_hdfs_on_network_type
then
    # Default block size is 1M in Lustre
    # XXX: If not default, can get from lctl or similar?
    # If other networkFS, just assume like Lustre
    iobuffersize=1048576
elif [ "${HADOOP_FILESYSTEM_MODE}" == "rawnetworkfs" ]
then
    # Assuming Lustre, so copy above 1M
    iobuffersize=1048576
elif [ "${HADOOP_FILESYSTEM_MODE}" == "intellustre" ]
then
    # Assuming Lustre, so copy above 1M
    iobuffersize=1048576
fi 
Hamster_print iobuffersize "I/O Buffer Size(B)"

# Sets hamster_processor_count & hamster_threads_to_use
Hamster_calculate_threads_to_use

# Sets hamster_memory_to_use
Hamster_calculate_memory_to_use

if [ "${HADOOP_MAX_TASKS_PER_NODE}X" != "X" ]
then
    maxtaskspernode=${HADOOP_MAX_TASKS_PER_NODE}
else
    maxtaskspernode=${hamster_threads_to_use}
fi
Hamster_print maxtaskspernode "Max Tasks Per Node"

if [ "${YARN_RESOURCE_MEMORY}X" != "X" ]
then
    yarnresourcememory=${YARN_RESOURCE_MEMORY}
else
    yarnresourcememory=${hamster_memory_to_use}
fi
Hamster_print yarnresourcememory "Yarn Resource Memory"

if [ "${HADOOP_CHILD_HEAPSIZE}X" != "X" ]
then
    allchildheapsize=${HADOOP_CHILD_HEAPSIZE}
else
# achu: We round down to nearest 1024M
    tmp1=`echo "${yarnresourcememory} / ${maxtaskspernode}" | bc -l | xargs printf "%1.2f"`
    tmp2=`echo "${tmp1} / 1024" | bc -l | xargs printf "%1.0f"`
    allchildheapsize=`echo "${tmp2} * 1024" | bc -l | xargs printf "%1.0f"`
    if [ "${allchildheapsize}" -lt "1024" ]
    then
        allchildheapsize=1024
    fi
fi
Hamster_print allchildheapsize "All Child Heap Size(MB)"

if [ "${HADOOP_CHILD_MAP_HEAPSIZE}X" != "X" ]
then
    mapchildheapsize=${HADOOP_CHILD_MAP_HEAPSIZE}
else
    mapchildheapsize=${allchildheapsize}
fi
Hamster_print mapchildheapsize "Map child heap size"

if [ "${HADOOP_CHILD_REDUCE_HEAPSIZE}X" != "X" ]
then
    reducechildheapsize=${HADOOP_CHILD_REDUCE_HEAPSIZE}
else
    if Hamster_hadoop_is_MR 2 || Hamster_hadoop_is_MR 3
    then
        reducechildheapsize=`expr ${mapchildheapsize} \* 2`
    else
        reducechildheapsize=${allchildheapsize}
    fi
fi
Hamster_print reducechildheapsize "Reduce Child Heap Size(MB)"

if [ "${HADOOP_CHILD_MAP_CONTAINER_BUFFER}X" != "X" ]
then
    mapcontainerbuffer=${HADOOP_CHILD_MAP_CONTAINER_BUFFER}
else
    # Estimate 256M per G
    numgig=`expr ${mapchildheapsize} / 1024`
    if [ "${numgig}" == "0" ]
    then
        numgig=1
    fi
    mapcontainerbuffer=`expr ${numgig} \* 256`
fi
Hamster_print mapcontainerbuffer "Map Container Buffer(MB)"

if [ "${HADOOP_CHILD_REDUCE_CONTAINER_BUFFER}X" != "X" ]
then
    reducecontainerbuffer=${HADOOP_CHILD_REDUCE_CONTAINER_BUFFER}
else
    # Estimate 256M per G
    numgig=`expr ${reducechildheapsize} / 1024`
    if [ "${numgig}" == "0" ]
    then
        numgig=1
    fi
    reducecontainerbuffer=`expr ${numgig} \* 256`
fi
Hamster_print reducecontainerbuffer "Reduce Container Buffer(MB)"

# Cloudera recommends 256 for io.sort.mb.  Cloudera blog suggests
# io.sort.factor * 10 ~= io.sort.mb.

if [ "${HADOOP_IO_SORT_MB}X" != "X" ]
then
    iosortmb=${HADOOP_IO_SORT_MB}
else
    # 128M per gig
    numgig=`expr ${mapchildheapsize} / 1024`
    if [ "${numgig}" == "0" ]
    then
        numgig=1
    fi
    iosortmb=`expr ${numgig} \* 128`
fi
Hamster_print iosortmb "I/O Sort(MB)"

if [ "${HADOOP_IO_SORT_FACTOR}X" != "X" ]
then
    iosortfactor=${HADOOP_IO_SORT_FACTOR}
else
    iosortfactor=`expr ${iosortmb} \/ 10`
fi
Hamster_print iosortfactor "I/O Sort Factor"

mapcontainermb=`expr ${mapchildheapsize} + ${mapcontainerbuffer}`
reducecontainermb=`expr ${reducechildheapsize} + ${reducecontainerbuffer}`
Hamster_print mapcontainermb "Map Container(MB)"
Hamster_print reducecontainermb "Reduce Container(MB)"

yarnmincontainer=1024
if [ ${mapcontainermb} -lt ${yarnmincontainer} ]
then
    yarnmincontainer=${mapcontainermb}
fi

if [ ${reducecontainermb} -lt ${yarnmincontainer} ]
then
    yarnmincontainer=${reducecontainermb}
fi
Hamster_print yarnmincontainer "Yarn Min Container"

yarnmaxcontainer=${yarnresourcememory}

if [ ${reducecontainermb} -gt ${yarnmaxcontainer} ]
then
    yarnmaxcontainer=${reducecontainermb}
fi
Hamster_print yarnmaxcontainer "Yarn Max Container"

if [ "${HADOOP_MAPREDUCE_SLOWSTART}X" != "X" ]
then
    mapredslowstart=${HADOOP_MAPREDUCE_SLOWSTART}
else
    mapredslowstart="0.05"
fi
Hamster_print mapredslowstart "Map Reduce Slow Start"

if [ "${HADOOP_DEFAULT_MAP_TASKS}X" != "X" ]
then
    defaultmaptasks=${HADOOP_DEFAULT_MAP_TASKS}
else
    defaultmaptasks=`expr ${maxtaskspernode} \* ${HADOOP_SLAVE_COUNT}`
fi

if [ "${HADOOP_DEFAULT_REDUCE_TASKS}X" != "X" ]
then
    defaultreducetasks=${HADOOP_DEFAULT_REDUCE_TASKS}
else
    defaultreducetasks=${HADOOP_SLAVE_COUNT}
fi

if [ "${HADOOP_MAX_MAP_TASKS}X" != "X" ]
then
    maxmaptasks=${HADOOP_MAX_MAP_TASKS}
else
    maxmaptasks=${maxtaskspernode}
fi

if [ "${HADOOP_MAX_REDUCE_TASKS}X" != "X" ]
then
    maxreducetasks=${HADOOP_MAX_REDUCE_TASKS}
else
    maxreducetasks=${maxtaskspernode}
fi

if [ "${HADOOP_HDFS_BLOCKSIZE}X" != "X" ]
then
    hdfsblocksize=${HADOOP_HDFS_BLOCKSIZE}
else
    # 64M is Hadoop default, widely considered bad choice, we'll use 128M as default
    hdfsblocksize=134217728
fi

if [ "${HADOOP_HDFS_REPLICATION}X" != "X" ]
then
    hdfsreplication=${HADOOP_HDFS_REPLICATION}
else
    hdfsreplication=${default_hdfs_replication}
fi

# If not rawnetworkfs don't want to affect other filesystems, so need
# to check HADOOP_FILESYSTEM_MODE
if [ "${HADOOP_RAWNETWORKFS_BLOCKSIZE}X" != "X" ] \
    && [ "${HADOOP_FILESYSTEM_MODE}" == "rawnetworkfs" ]
then
    rawnetworkfsblocksize=${HADOOP_RAWNETWORKFS_BLOCKSIZE}
else
    # 32M is Hadoop default for local
    rawnetworkfsblocksize=33554432
fi

if [ "${HADOOP_INTELLUSTRE_BLOCKSIZE}X" != "X" ]
then
    intellustreblocksize=${HADOOP_INTELLUSTRE_BLOCKSIZE}
else
    intellustreblocksize=536870912
fi

if [ "${HADOOP_INTELLUSTRE_STRIPESIZE}X" != "X" ]
then
    intellustrestripesize=${HADOOP_INTELLUSTRE_STRIPESIZE}
else
    intellustrestripesize=134217728
fi

if [ "${HADOOP_INTELLUSTRE_STRIPECOUNT}X" != "X" ]
then
    intellustrestripecount=${HADOOP_INTELLUSTRE_STRIPECOUNT}
else
    intellustrestripecount=-1
fi

if [ "${HADOOP_FILESYSTEM_MODE}" == "intellustre" ] && [ "${HADOOP_INTELLUSTRE_SHUFFLE}X" != "X" ]
then
    if [ "${HADOOP_INTELLUSTRE_SHUFFLE}" == "yes" ]
    then
        mapoutputcollectorclass="org.apache.hadoop.mapred.SharedFsPlugins\$MapOutputBuffer"
        reduceshuffleconsumerplugin="org.apache.hadoop.mapred.SharedFsPlugins\$Shuffle"
    else
        mapoutputcollectorclass="org.apache.hadoop.mapred.MapTask\$MapOutputBuffer"
        reduceshuffleconsumerplugin="org.apache.hadoop.mapreduce.task.reduce.Shuffle"
    fi
else
    mapoutputcollectorclass="org.apache.hadoop.mapred.MapTask\$MapOutputBuffer"
    reduceshuffleconsumerplugin="org.apache.hadoop.mapreduce.task.reduce.Shuffle"
fi

if [ "${HADOOP_DAEMON_HEAP_MAX}X" != "X" ]
then
    hadoopdaemonheapmax="${HADOOP_DAEMON_HEAP_MAX}"
else
    hadoopdaemonheapmax="10240"
fi 

if [ "${HADOOP_NAMENODE_DAEMON_HEAP_MAX}X" != "X" ]
then
    hadoopnamenodedaemonheapmax="${HADOOP_NAMENODE_DAEMON_HEAP_MAX}"
else
    hadoopnamenodedaemonheapmax="${hadoopdaemonheapmax}"
fi 

if [ "${HADOOP_COMPRESSION}X" != "X" ]
then
    if [ "${HADOOP_COMPRESSION}" == "yes" ]
    then
        compression=true
    else
        compression=false
    fi
else
    compression=false
fi

# Sets hamster_openfilescount
Hamster_calculate_openfiles ${HADOOP_SLAVE_COUNT}
# Sets hamster_userprocessescount
Hamster_calculate_userprocesses ${HADOOP_SLAVE_COUNT}

#
# Setup Hadoop file system
#

pathcount=0

# sets hamster_hadooptmpdir and hamster_fsdefault
Hamster_calculate_hadoop_filesystem_paths ${hadoopnoderank}

if [ "${HADOOP_FILESYSTEM_MODE}" == "hdfs" ]
then
    hamster_hadooptmpdirsubst=`echo "${hamster_hadooptmpdir}" | sed "s/\\//\\\\\\\\\//g"`
    hamster_fsdefaultsubst=`echo ${hamster_fsdefault} | sed "s/\\//\\\\\\\\\//g"`

    IFSORIG=${IFS}
    IFS=","
    datanodedirtmp=""
    for hdfspath in ${HADOOP_HDFS_PATH}
    do
        if [ ! -d "${hdfspath}" ]
        then
            Hamster_mkdir ${hdfspath}
        fi

        version=$(Hamster_hadoop_version)
        if [ "${version}" == "1" ]
        then
            datanodedirtmp="${datanodedirtmp}${datanodedirtmp:+","}${hdfspath}/dfs/data"
        elif [ "${version}" == "2" ]
        then
            datanodedirtmp="${datanodedirtmp}${datanodedirtmp:+","}file://${hdfspath}/dfs/data"
        elif [ "${version}" == "3" ]
        then
            datanodedirtmp="${datanodedirtmp}${datanodedirtmp:+","}file://${hdfspath}/dfs/data"
        fi
        pathcount=`expr ${pathcount} + 1`
    done
    IFS=${IFSORIG}
    datanodedir=`echo "${datanodedirtmp}" | sed "s/\\//\\\\\\\\\//g"`
elif Hamster_hadoop_filesystem_mode_is_hdfs_on_network_type
then
    hamster_hadooptmpdirsubst=`echo "${hamster_hadooptmpdir}" | sed "s/\\//\\\\\\\\\//g"`
    hamster_fsdefaultsubst=`echo ${hamster_fsdefault} | sed "s/\\//\\\\\\\\\//g"`
    
    if [ ! -d "${hamster_hadooptmpdir}" ]
    then
        Hamster_mkdir ${hamster_hadooptmpdir}

        if [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsoverlustre" ]
        then
            lfscmd=`which lfs`
            if [ $? -eq 0 ] ; then
                ${lfscmd} setstripe --size ${hdfsblocksize} --count 1 ${hamster_hadooptmpdir}
            else
                Hamster_output_internal_warning "Can't find lfs and set stripe size"
            fi
        fi
    fi
    
    version=$(Hamster_hadoop_version)
    if [ "${version}" == "1" ]
    then
        datanodedir="$\{hadoop.tmp.dir\}\/dfs\/data"
    elif [ "${version}" == "2" ]
    then
        datanodedir="file:\/\/\$\{hadoop.tmp.dir\}\/dfs\/data"
    elif [ "${version}" == "3" ]
    then
        datanodedir="file:\/\/\$\{hadoop.tmp.dir\}\/dfs\/data"
    fi
    
    pathcount=1

    # Cleanup locks if requested
    if ([ "${HADOOP_FILESYSTEM_MODE}" == "hdfsoverlustre" ] && [ "${HADOOP_HDFSOVERLUSTRE_REMOVE_LOCKS}" == "yes" ]) \
        || ([ "${HADOOP_FILESYSTEM_MODE}" == "hdfsovernetworkfs" ] && [ "${HADOOP_HDFSOVERNETWORKFS_REMOVE_LOCKS}" == "yes" ])
    then
        for num in `seq 0 ${HADOOP_SLAVE_COUNT}`
        do
            if [ "${HADOOP_PER_JOB_HDFS_PATH}" == "yes" ]
            then
                lockfile="${HADOOP_HDFSOVERLUSTRE_PATH}/${HAMSTER_JOB_ID}/node-${num}/dfs/data/in_use.lock"
            else
                lockfile="${HADOOP_HDFSOVERLUSTRE_PATH}/node-${num}/dfs/data/in_use.lock"
            fi
            rm -f ${lockfile}
        done
    fi
elif [ "${HADOOP_FILESYSTEM_MODE}" == "rawnetworkfs" ]
then
    hamster_hadooptmpdirsubst=`echo "${hamster_hadooptmpdir}" | sed "s/\\//\\\\\\\\\//g"`
    hamster_fsdefaultsubst=`echo ${hamster_fsdefault} | sed "s/\\//\\\\\\\\\//g"`
    
    if [ ! -d "${HADOOP_RAWNETWORKFS_PATH}" ]
    then
        Hamster_mkdir ${HADOOP_RAWNETWORKFS_PATH}
    fi
        
    if [ ! -d "${hamster_hadooptmpdir}" ]
    then
        Hamster_mkdir ${hamster_hadooptmpdir}
    fi
    
    pathcount=1
elif [ "${HADOOP_FILESYSTEM_MODE}" == "intellustre" ]
then
    intellustrerootdir=`echo "${HADOOP_INTELLUSTRE_PATH}" | sed "s/\\//\\\\\\\\\//g"`
    hamster_hadooptmpdirsubst=`echo "${hamster_hadooptmpdir}" | sed "s/\\//\\\\\\\\\//g"`
    hamster_fsdefaultsubst=`echo ${hamster_fsdefault} | sed "s/\\//\\\\\\\\\//g"`

    if [ ! -d "${hamster_hadooptmpdir}" ]
    then
        Hamster_mkdir ${hamster_hadooptmpdir}
    fi

    pathcount=1
else
    Hamster_output_internal_error "Illegal HADOOP_FILESYSTEM_MODE \"${HADOOP_FILESYSTEM_MODE}\" specified"
    exit 1
fi

if ([ "${HADOOP_FILESYSTEM_MODE}" == "hdfsoverlustre" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsovernetworkfs" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "rawnetworkfs" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "intellustre" ]) \
    && [ "${HADOOP_LOCALSTORE}X" != "X" ]
then
    IFSORIG=${IFS}
    IFS=","
    mapredlocalstoredir=""
    yarnlocalstoredir=""
    for localstorefile in ${HADOOP_LOCALSTORE}
    do
        localstoredirtmp=${localstorefile}
        mapredlocalstoredirtmp=`echo "${localstoredirtmp}/mapred/local" | sed "s/\\//\\\\\\\\\//g"`
        yarnlocalstoredirtmp=`echo "${localstoredirtmp}/yarn-nm" | sed "s/\\//\\\\\\\\\//g"`
        
        if [ ! -d "${localstoredirtmp}" ]
        then
            Hamster_mkdir ${localstoredirtmp}
        fi

        mapredlocalstoredir="${mapredlocalstoredir}${mapredlocalstoredir:+","}${mapredlocalstoredirtmp}"
        yarnlocalstoredir="${yarnlocalstoredir}${yarnlocalstoredir:+","}${yarnlocalstoredirtmp}"
    done
    IFS=${IFSORIG}
elif [ "${HADOOP_FILESYSTEM_MODE}" == "hdfs" ] \
    && [ ${pathcount} -gt 1 ] 
then
    IFSORIG=${IFS}
    IFS=","
    mapredlocalstoredir=""
    yarnlocalstoredir=""
    for localstorefile in ${HADOOP_HDFS_PATH}
    do
        localstoredirtmp=${localstorefile}
        mapredlocalstoredirtmp=`echo "${localstoredirtmp}/mapred/local" | sed "s/\\//\\\\\\\\\//g"`
        yarnlocalstoredirtmp=`echo "${localstoredirtmp}/yarn-nm" | sed "s/\\//\\\\\\\\\//g"`
        
        mapredlocalstoredir="${mapredlocalstoredir}${mapredlocalstoredir:+","}${mapredlocalstoredirtmp}"
        yarnlocalstoredir="${yarnlocalstoredir}${yarnlocalstoredir:+","}${yarnlocalstoredirtmp}"
    done
    IFS=${IFSORIG}
else
    mapredlocalstoredir="\$\{hadoop.tmp.dir\}\/mapred\/local"
    yarnlocalstoredir="\$\{hadoop.tmp.dir\}\/yarn-nm"
fi

if Hamster_hadoop_filesystem_mode_is_hdfs_type
then
    # If HDFS would be stored in HDFS, so pick a better path
    yardappmapreduceamstagingdir="${HADOOP_LOCAL_SCRATCHSPACE_DIR}/yarn/"
else
    yardappmapreduceamstagingdir="${hamster_hadooptmpdir}/yarn/"
fi

# Sets hamster_hadoop_stop_timeout
Hamster_calculate_stop_timeouts

# set java.io.tmpdir
extrahadoopopts="${extrahadoopopts}${extrahadoopopts:+" "}-Djava.io.tmpdir=${HADOOP_LOCAL_SCRATCHSPACE_DIR}/tmp"

extrayarnopts="${extrayarnopts}${extrayarnopts+" "}-Djava.io.tmpdir=${YARN_LOCAL_SCRATCHSPACE_DIR}/tmp"

if [ ! -d "${HADOOP_LOCAL_SCRATCHSPACE_DIR}/tmp" ]
then
    Hamster_mkdir ${HADOOP_LOCAL_SCRATCHSPACE_DIR}/tmp
fi

if [ ! -d "${YARN_LOCAL_SCRATCHSPACE_DIR}/tmp" ]
then
    Hamster_mkdir ${YARN_LOCAL_SCRATCHSPACE_DIR}/tmp
fi

# disable hsperfdata if using NO_LOCAL_DIR
if [ "${HAMSTER_NO_LOCAL_DIR}" == "yes" ]
then
    extrahadoopopts="${extrahadoopopts}${extrahadoopopts:+" "}-XX:-UsePerfData"
    
    extrayarnopts="${extrayarnopts}${extrayarnopts:+" "}-XX:-UsePerfData"
fi

#
# Get config files for setup
#

# Hamster_find_conffile will set the 'pre' filenames

version=$(Hamster_hadoop_version)
Hamster_print version
if [ "$version" == "1" ]
then
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "core-site.xml" "pre_coresitexml" 
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "mapred-site.xml" "pre_mapredsitexml"
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "hadoop-env.sh" "pre_hadoopenvsh"
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "hdfs-site.xml" "pre_hdfssitexml"
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "hadoop.log4j.properties" "pre_log4jproperties"
elif [ "$version" == "2" ]
then
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "core-site.xml" "pre_coresitexml"
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "mapred-site.xml" "pre_mapredsitexml"
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "hadoop-env.sh" "pre_hadoopenvsh"
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "hdfs-site.xml" "pre_hdfssitexml"
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "hadoop.log4j.properties" "pre_log4jproperties"

    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "mapred-env.sh" "pre_mapredenvsh"
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "yarn-site.xml" "pre_yarnsitexml"
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "yarn-env.sh" "pre_yarnenvsh"

    if [ "${HADOOP_FILESYSTEM_MODE}" == "intellustre" ]
    then
        Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "core-site-intellustre.xml" "pre_coresitexml_intellustre"
    fi

    if Hamster_hdfs_federation_enabled
    then
        Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "core-site-viewfsmount.xml" "pre_coresitexml_viewfsmount"
    fi
elif [ "$version" == "3" ]
then
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "core-site.xml" "pre_coresitexml"
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "mapred-site.xml" "pre_mapredsitexml"
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "hadoop-env.sh" "pre_hadoopenvsh"
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "hdfs-site.xml" "pre_hdfssitexml"
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "hadoop.log4j.properties" "pre_log4jproperties"

    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "mapred-env.sh" "pre_mapredenvsh"
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "yarn-site.xml" "pre_yarnsitexml"
    Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "yarn-env.sh" "pre_yarnenvsh"

    if [ "${HADOOP_FILESYSTEM_MODE}" == "intellustre" ]
    then
        Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "core-site-intellustre.xml" "pre_coresitexml_intellustre"
    fi

    if Hamster_hdfs_federation_enabled
    then
        Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "core-site-viewfsmount.xml" "pre_coresitexml_viewfsmount"
    fi
else
    Hamster_output_internal_error "Illegal HADOOP_SETUP_TYPE \"${HADOOP_SETUP_TYPE}\" specified"
    exit 1
fi

post_coresitexml=${HADOOP_CONF_DIR}/core-site.xml
post_mapredsitexml=${HADOOP_CONF_DIR}/mapred-site.xml
post_hadoopenvsh=${HADOOP_CONF_DIR}/hadoop-env.sh
post_mapredenvsh=${HADOOP_CONF_DIR}/mapred-env.sh
post_yarnsitexml=${HADOOP_CONF_DIR}/yarn-site.xml
post_yarnenvsh=${HADOOP_CONF_DIR}/yarn-env.sh
post_hdfssitexml=${HADOOP_CONF_DIR}/hdfs-site.xml
post_log4jproperties=${HADOOP_CONF_DIR}/log4j.properties

javahomesubst=`echo "${JAVA_HOME}" | sed "s/\\//\\\\\\\\\//g"`
extrahadoopoptssubst=`echo "${extrahadoopopts}" | sed "s/\\//\\\\\\\\\//g"`

#
# Setup Hadoop configuration files and environment files
#

if Hamster_hadoop_setup_type_is_valid ${HADOOP_SETUP_TYPE}
then
    Hamster_echo "HADOOP_SETUP_TYPE is valid"
    cp ${pre_coresitexml} ${post_coresitexml}

    if [ "${pre_coresitexml_intellustre}X" != "X" ]
    then
        sed -i -e "/@INTELLUSTRE@/{r ${pre_coresitexml_intellustre}" -e "d}" ${post_coresitexml}
    else
        sed -i -e "/@INTELLUSTRE@/,+1d" ${post_coresitexml}
    fi

    if [ "${pre_coresitexml_viewfsmount}X" != "X" ]
    then
        for i in `seq 1 ${HDFS_FEDERATION_NAMENODE_COUNT}`
        do
            j=1
            while true
            do
                mountname="HDFS_FEDERATION_NAMENODE_${i}_MOUNT_${j}"
                eval mountvalue="\$$mountname"

                if [ "${mountvalue}X" == "X" ]
                then
                    break
                fi
                
                sed -i -e "/@VIEWFSMOUNT@/{r ${pre_coresitexml_viewfsmount}" -e "d}" ${post_coresitexml}

                # First namenode is based on master
                if [ "${i}" == "1" ]
                then
                    federationnamenodehost="${HADOOP_MASTER_NODE}"
                else
                    numline=`expr ${i} - 1`
                    federationnamenodehost=`sed -n "${numline}p" ${HADOOP_CONF_DIR}/namenode_hdfs_federation`
                fi

                mountvaluesubst=`echo "${mountvalue}" | sed "s/\\//\\\\\\\\\//g"`
                
                sed -i -e "s/VIEWFSPATH/${mountvaluesubst}/g" ${post_coresitexml}
                sed -i -e "s/VIEWFSNAMENODE/${federationnamenodehost}/g" ${post_coresitexml}
                sed -i -e "s/NAMESERVICEID/ns${i}/g" ${post_coresitexml}
                sed -i -e "s/DFSNAMENODERPCADDRESS/${default_hadoop_hdfs_namenode_address}/g" ${post_coresitexml}

                j=`expr ${j} + 1`
            done
        done

        sed -i -e "/@VIEWFSMOUNT@/,+1d" ${post_coresitexml}
        
    else
        sed -i -e "/@VIEWFSMOUNT@/,+1d" ${post_coresitexml}
    fi

    sed -i \
        -e "s/HADOOPTMPDIR/${hamster_hadooptmpdirsubst}/g" \
        -e "s/FSDEFAULT/${hamster_fsdefaultsubst}/g" \
        -e "s/IOBUFFERSIZE/${iobuffersize}/g" \
        -e "s/LOCALBLOCKSIZE/${rawnetworkfsblocksize}/g" \
        -e "s/INTELLUSTREROOTDIR/${intellustrerootdir}/g" \
        -e "s/INTELLUSTREBLOCKSIZE/${intellustreblocksize}/g" \
        -e "s/INTELLUSTRESTRIPESIZE/${intellustrestripesize}/g" \
        -e "s/INTELLUSTRESTRIPECOUNT/${intellustrestripecount}/g" \
        -e "s/HADOOPDEFAULTUSER/${USER}/g" \
        ${post_coresitexml}

    hadoopconfdirsubst=`echo "${HADOOP_CONF_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    hadooplogdirsubst=`echo "${HADOOP_LOG_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    hadooppiddirsubst=`echo "${HADOOP_PID_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopmapredpiddirsubst=`echo "${HADOOP_MAPRED_PID_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    hadoophomesubst=`echo "${HADOOP_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopcommonhomesubst=`echo "${HADOOP_COMMON_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopmapredhomesubst=`echo "${HADOOP_MAPRED_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoophdfshomesubst=`echo "${HADOOP_HDFS_HOME}" | sed "s/\\//\\\\\\\\\//g"`

    extrahadoopclassessubst=`echo "${extrahadoopclasses}" | sed "s/\\//\\\\\\\\\//g"`

    cp ${pre_hadoopenvsh} ${post_hadoopenvsh}

    sed -i \
        -e "s/HADOOP_JAVA_HOME/${javahomesubst}/g" \
        -e "s/HADOOP_DAEMON_HEAP_MAX/${hadoopdaemonheapmax}/g" \
        -e "s/HADOOP_NAMENODE_HEAP_MAX/${hadoopnamenodedaemonheapmax}/g" \
        -e "s/HADOOPTIMEOUTSECONDS/${hamster_hadoop_stop_timeout}/g" \
        -e "s/HADOOPCONFDIR/${hadoopconfdirsubst}/g" \
        -e "s/HADOOPLOGDIR/${hadooplogdirsubst}/g" \
        -e "s/HADOOPPIDDIR/${hadooppiddirsubst}/g" \
        -e "s/HADOOPMAPREDPIDDIR/${hadoopmapredpiddirsubst}/g" \
        -e "s/HADOOPHOME/${hadoophomesubst}/g" \
        -e "s/HADOOPCOMMONHOME/${hadoopcommonhomesubst}/g" \
        -e "s/HADOOPMAPREDHOME/${hadoopmapredhomesubst}/g" \
        -e "s/HADOOPHDFSHOME/${hadoophdfshomesubst}/g" \
        -e "s/EXTRAHADOOPCLASSES/${extrahadoopclassessubst}/g" \
        -e "s/EXTRAHADOOPOPTS/${extrahadoopoptssubst}/g" \
        ${post_hadoopenvsh}

    if [ "${HADOOP_ENVIRONMENT_EXTRA_PATH}X" != "X" ] && [ -f ${HADOOP_ENVIRONMENT_EXTRA_PATH} ]
    then
        cat ${HADOOP_ENVIRONMENT_EXTRA_PATH} >> ${post_hadoopenvsh}
    else
        echo "ulimit -n ${hamster_openfilescount}" >> ${post_hadoopenvsh}
        echo "ulimit -u ${hamster_userprocessescount}" >> ${post_hadoopenvsh}
    fi

    cp ${pre_log4jproperties} ${post_log4jproperties}
fi

# HADOOP CONF
hadoopmapredlogdirsubst=`echo "${HADOOP_LOG_DIR}" | sed "s/\\//\\\\\\\\\//g"`
hadoopconfdirsubst=`echo "${HADOOP_CONF_DIR}" | sed "s/\\//\\\\\\\\\//g"`
hadoophomesubst=`echo "${HADOOP_HOME}" | sed "s/\\//\\\\\\\\\//g"`
hadoopcommonhomesubst=`echo "${HADOOP_COMMON_HOME}" | sed "s/\\//\\\\\\\\\//g"`
hadoopmapredhomesubst=`echo "${HADOOP_MAPRED_HOME}" | sed "s/\\//\\\\\\\\\//g"`

if [ "$(Hamster_hadoop_type)" == "MR" ]
then
    yardappmapreduceamstagingdirsubst=`echo "${yardappmapreduceamstagingdir}" | sed "s/\\//\\\\\\\\\//g"`
    extrahadooptaskoptssubst=`echo "${extrahadooptaskopts}" | sed "s/\\//\\\\\\\\\//g"`

    cp ${pre_mapredsitexml} ${post_mapredsitexml}

    # Space w/ HADOOPEXTRATASKJAVAOPTS & EXTRAHADOOPOPTS substitution is intentional
    sed -i \
        -e "s/HADOOP_MASTER_HOST/${HADOOP_MASTER_NODE}/g" \
        -e "s/MRPARALLELCOPIES/${parallelcopies}/g" \
        -e "s/JOBTRACKERHANDLERCOUNT/${jobtrackerhandlercount}/g" \
        -e "s/MRSLOWSTART/${mapredslowstart}/g" \
        -e "s/ALLCHILDHEAPSIZE/${allchildheapsize}/g" \
        -e "s/MAPCHILDHEAPSIZE/${mapchildheapsize}/g" \
        -e "s/MAPCONTAINERMB/${mapcontainermb}/g" \
        -e "s/REDUCECHILDHEAPSIZE/${reducechildheapsize}/g" \
        -e "s/REDUCECONTAINERMB/${reducecontainermb}/g" \
        -e "s/DEFAULTMAPTASKS/${defaultmaptasks}/g" \
        -e "s/DEFAULTREDUCETASKS/${defaultreducetasks}/" \
        -e "s/MAXMAPTASKS/${maxmaptasks}/g" \
        -e "s/MAXREDUCETASKS/${maxreducetasks}/g" \
        -e "s/LOCALSTOREDIR/${mapredlocalstoredir}/g" \
        -e "s/IOSORTFACTOR/${iosortfactor}/g" \
        -e "s/IOSORTMB/${iosortmb}/g" \
        -e "s/HADOOPCOMPRESSION/${compression}/g" \
        -e "s/HADOOPHOSTSINCLUDEFILENAME/${hostsincludefilesubst}/g" \
        -e "s/HADOOPHOSTSEXCLUDEFILENAME/${hostsexcludefilesubst}/g" \
        -e "s/SUBMITFILEREPLICATION/${submitfilereplication}/g" \
        -e "s/MAPOUTPUTCOLLECTORCLASS/${mapoutputcollectorclass}/g" \
        -e "s/REDUCESHUFFLECONSUMERPLUGIN/${reduceshuffleconsumerplugin}/g" \
        -e "s/RDMABUFSIZE/${rdmabufsize}/g" \
        -e "s/HADOOPJOBHISTORYADDRESS/${default_hadoop_jobhistoryserver_address}/g" \
        -e "s/HADOOPJOBHISTORYWEBAPPADDRESS/${default_hadoop_jobhistoryserver_webapp_address}/g" \
        -e "s/MAPREDJOBTRACKERADDRESS/${default_mapred_job_tracker_address}/g" \
        -e "s/MAPREDJOBTRACKERHTTPADDRESS/${default_mapred_job_tracker_httpaddress}/g" \
        -e "s/YARNAPPMAPREDUCEAMSTAGINGDIR/${yardappmapreduceamstagingdirsubst}/g" \
        -e "s/HADOOPEXTRATASKJAVAOPTS/ ${extrahadooptaskoptssubst}/g" \
        -e "s/EXTRAHADOOPOPTS/ ${extrahadoopoptssubst}/g" \
        -e "s/HADOOPMAPREDHOME/${hadoopmapredhomesubst}/g" \
        ${post_mapredsitexml}
fi

if Hamster_hadoop_is_MR 2
then
    #
    # A few settings depend on version
    #

    # Returns 0 for ==, 1 for $1 > $2, 2 for $1 < $2
    Hamster_vercomp ${HADOOP_VERSION} "2.2.0"
    if [ $? == "2" ]
    then 
        yarnauxservices="mapreduce.shuffle"
        yarnauxmapreduceshuffle="yarn.nodemanager.aux-services.mapreduce.shuffle.class"
    else
        yarnauxservices="mapreduce_shuffle"
        yarnauxmapreduceshuffle="yarn.nodemanager.aux-services.mapreduce_shuffle.class"
    fi

    Hamster_print yarnauxservices "YARN Aux Services"
    Hamster_print yarnauxmapreduceshuffle "YARN Aux MapReduce Shuffle"

    cp ${pre_yarnsitexml} ${post_yarnsitexml}

    sed -i \
        -e "s/HADOOP_MASTER_HOST/${HADOOP_MASTER_NODE}/g" \
        -e "s/YARNMINCONTAINER/${yarnmincontainer}/g" \
        -e "s/YARNMAXCONTAINER/${yarnmaxcontainer}/g" \
        -e "s/YARNRESOURCEMEMORY/${yarnresourcememory}/g" \
        -e "s/LOCALSTOREDIR/${yarnlocalstoredir}/g" \
        -e "s/YARNAUXSERVICES/${yarnauxservices}/g" \
        -e "s/YARNAUXMAPREDUCESHUFFLE/${yarnauxmapreduceshuffle}/g" \
        -e "s/YARNRESOURCEMANAGERADDRESS/${default_yarn_resourcemanager_address}/g" \
        -e "s/YARNRESOURCEMANAGERSCHEDULERADDRESS/${default_yarn_resourcemanager_scheduler_address}/g" \
        -e "s/YARNRESOURCEMANAGERWEBAPPADDRESS/${default_yarn_resourcemanager_webapp_address}/g" \
        -e "s/YARNRESOURCEMANAGERWEBAPPHTTPSADDRESS/${default_yarn_resourcemanager_webapp_https_address}/g" \
        -e "s/YARNRESOURCEMANAGERRESOURCETRACKERADDRESS/${default_yarn_resourcemanager_resourcetracker_address}/g" \
        -e "s/YARNRESOURCEMANAGERADMINADDRESS/${default_yarn_resourcemanager_admin_address}/g" \
        -e "s/YARNLOCALIZERADDRESS/${default_yarn_nodemanager_localizer_address}/g" \
        -e "s/YARNNODEMANAGERWEBAPPADDRESS/${default_yarn_nodemanager_webapp_address}/g" \
        -e "s/YARNDEFAULTUSER/${USER}/g" \
        -e "s/YARNRESROUCECPUVCORES/${hamster_processor_count}/g" \
        -e "s/HADOOPMAPREDHOME/${hadoopmapredhomesubst}/g" \
        ${post_yarnsitexml}
    
    cp ${pre_mapredenvsh} ${post_mapredenvsh}

    sed -i \
        -e "s/HADOOP_JAVA_HOME/${javahomesubst}/g" \
        -e "s/HADOOP_DAEMON_HEAP_MAX/${hadoopdaemonheapmax}/g" \
        -e "s/HADOOPTIMEOUTSECONDS/${hamster_hadoop_stop_timeout}/g" \
        -e "s/HADOOPMAPREDLOGDIR/${hadoopmapredlogdirsubst}/g" \
        -e "s/HADOOPCONFDIR/${hadoopconfdirsubst}/g" \
        -e "s/HADOOPHOME/${hadoophomesubst}/g" \
        -e "s/HADOOPCOMMONHOME/${hadoopcommonhomesubst}/g" \
        -e "s/HADOOPMAPREDHOME/${hadoopmapredhomesubst}/g" \
        ${post_mapredenvsh}

    yarnconfdirsubst=`echo "${YARN_CONF_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    yarnlogdirsubst=`echo "${YARN_LOG_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    yarnpiddirsubst=`echo "${YARN_PID_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    yarncommonhomesubst=`echo "${YARN_COMMON_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopyarnhomesubst=`echo "${HADOOP_YARN_HOME}" | sed "s/\\//\\\\\\\\\//g"`

    extrayarnlibrarypathsubst=`echo "${extrayarnlibrarypath}" | sed "s/\\//\\\\\\\\\//g"`
    extrayarnoptssubst=`echo "${extrayarnopts}" | sed "s/\\//\\\\\\\\\//g"`

    cp ${pre_yarnenvsh} ${post_yarnenvsh}

    sed -i \
        -e "s/HADOOP_JAVA_HOME/${javahomesubst}/g" \
        -e "s/HADOOP_DAEMON_HEAP_MAX/${hadoopdaemonheapmax}/g" \
        -e "s/HADOOPTIMEOUTSECONDS/${hamster_hadoop_stop_timeout}/g" \
        -e "s/YARNUSERNAME/${HADOOP_YARN_USER}/g" \
        -e "s/YARNCONFDIR/${yarnconfdirsubst}/g" \
        -e "s/YARNLOGDIR/${yarnlogdirsubst}/g" \
        -e "s/YARNPIDDIR/${yarnpiddirsubst}/g" \
        -e "s/YARNCOMMONHOME/${yarncommonhomesubst}/g" \
        -e "s/HADOOPYARNHOME/${hadoopyarnhomesubst}/g" \
        -e "s/EXTRAYARNLIBRARYPATH/${extrayarnlibrarypathsubst}/g" \
        -e "s/EXTRAYARNOPTS/${extrayarnoptssubst}/g" \
        ${post_yarnenvsh}

    if [ "${HADOOP_ENVIRONMENT_EXTRA_PATH}X" != "X" ] && [ -f ${HADOOP_ENVIRONMENT_EXTRA_PATH} ]
    then
        cat ${HADOOP_ENVIRONMENT_EXTRA_PATH} >> ${post_yarnenvsh}
    else
        echo "ulimit -n ${hamster_openfilescount}" >> ${post_yarnenvsh}
        echo "ulimit -u ${hamster_userprocessescount}" >> ${post_yarnenvsh}
    fi
fi

if Hamster_hadoop_is_MR 3
then
    #
    # A few settings depend on version
    #

    yarnauxservices="mapreduce_shuffle"
    yarnauxmapreduceshuffle="yarn.nodemanager.aux-services.mapreduce_shuffle.class"

    Hamster_print yarnauxservices "YARN Aux Services"
    Hamster_print yarnauxmapreduceshuffle "YARN Aux MapReduce Shuffle"

    cp ${pre_yarnsitexml} ${post_yarnsitexml}

    sed -i \
        -e "s/HADOOP_MASTER_HOST/${HADOOP_MASTER_NODE}/g" \
        -e "s/YARNMINCONTAINER/${yarnmincontainer}/g" \
        -e "s/YARNMAXCONTAINER/${yarnmaxcontainer}/g" \
        -e "s/YARNRESOURCEMEMORY/${yarnresourcememory}/g" \
        -e "s/LOCALSTOREDIR/${yarnlocalstoredir}/g" \
        -e "s/YARNAUXSERVICES/${yarnauxservices}/g" \
        -e "s/YARNAUXMAPREDUCESHUFFLE/${yarnauxmapreduceshuffle}/g" \
        -e "s/YARNRESOURCEMANAGERADDRESS/${default_yarn_resourcemanager_address}/g" \
        -e "s/YARNRESOURCEMANAGERSCHEDULERADDRESS/${default_yarn_resourcemanager_scheduler_address}/g" \
        -e "s/YARNRESOURCEMANAGERWEBAPPADDRESS/${default_yarn_resourcemanager_webapp_address}/g" \
        -e "s/YARNRESOURCEMANAGERWEBAPPHTTPSADDRESS/${default_yarn_resourcemanager_webapp_https_address}/g" \
        -e "s/YARNRESOURCEMANAGERRESOURCETRACKERADDRESS/${default_yarn_resourcemanager_resourcetracker_address}/g" \
        -e "s/YARNRESOURCEMANAGERADMINADDRESS/${default_yarn_resourcemanager_admin_address}/g" \
        -e "s/YARNLOCALIZERADDRESS/${default_yarn_nodemanager_localizer_address}/g" \
        -e "s/YARNNODEMANAGERWEBAPPADDRESS/${default_yarn_nodemanager_webapp_address}/g" \
        -e "s/YARNDEFAULTUSER/${USER}/g" \
        -e "s/YARNRESROUCECPUVCORES/${hamster_processor_count}/g" \
        -e "s/HADOOPMAPREDHOME/${hadoopmapredhomesubst}/g" \
        ${post_yarnsitexml}
    
    cp ${pre_mapredenvsh} ${post_mapredenvsh}

    sed -i \
        -e "s/HADOOP_JAVA_HOME/${javahomesubst}/g" \
        -e "s/HADOOP_DAEMON_HEAP_MAX/${hadoopdaemonheapmax}/g" \
        -e "s/HADOOPTIMEOUTSECONDS/${hamster_hadoop_stop_timeout}/g" \
        -e "s/HADOOPCONFDIR/${hadoopconfdirsubst}/g" \
        -e "s/HADOOPHOME/${hadoophomesubst}/g" \
        -e "s/HADOOPCOMMONHOME/${hadoopcommonhomesubst}/g" \
        -e "s/HADOOPMAPREDHOME/${hadoopmapredhomesubst}/g" \
        ${post_mapredenvsh}

    hadoopyarnhomesubst=`echo "${HADOOP_YARN_HOME}" | sed "s/\\//\\\\\\\\\//g"`

    extrayarnlibrarypathsubst=`echo "${extrayarnlibrarypath}" | sed "s/\\//\\\\\\\\\//g"`
    extrayarnoptssubst=`echo "${extrayarnopts}" | sed "s/\\//\\\\\\\\\//g"`

    cp ${pre_yarnenvsh} ${post_yarnenvsh}

    sed -i \
        -e "s/HADOOP_JAVA_HOME/${javahomesubst}/g" \
        -e "s/HADOOP_DAEMON_HEAP_MAX/${hadoopdaemonheapmax}/g" \
        -e "s/HADOOPTIMEOUTSECONDS/${hamster_hadoop_stop_timeout}/g" \
        -e "s/YARNUSERNAME/${HADOOP_YARN_USER}/g" \
        -e "s/HADOOPYARNHOME/${hadoopyarnhomesubst}/g" \
        -e "s/EXTRAYARNLIBRARYPATH/${extrayarnlibrarypathsubst}/g" \
        -e "s/EXTRAYARNOPTS/${extrayarnoptssubst}/g" \
        ${post_yarnenvsh}

    if [ "${HADOOP_ENVIRONMENT_EXTRA_PATH}X" != "X" ] && [ -f ${HADOOP_ENVIRONMENT_EXTRA_PATH} ]
    then
        cat ${HADOOP_ENVIRONMENT_EXTRA_PATH} >> ${post_yarnenvsh}
    else
        echo "ulimit -n ${hamster_openfilescount}" >> ${post_yarnenvsh}
        echo "ulimit -u ${hamster_userprocessescount}" >> ${post_yarnenvsh}
    fi
fi

if Hamster_hadoop_filesystem_mode_is_hdfs_type
then
    cp ${pre_hdfssitexml} ${post_hdfssitexml}

    if Hamster_hdfs_federation_enabled
    then
        Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "hdfs-site-hdfs-federation.xml" "pre_hdfssitexml_namenode"
    else
        Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "hdfs-site-hdfs-namenode.xml" "pre_hdfssitexml_namenode"
    fi

    sed -i -e "/@NAMENODE@/{r ${pre_hdfssitexml_namenode}" -e "d}" ${post_hdfssitexml}

    if Hamster_hdfs_federation_enabled
    then
        nameservices="ns1"
        for i in `seq 2 ${HDFS_FEDERATION_NAMENODE_COUNT}`
        do
            nameservices="${nameservices},ns${i}"
        done

        sed -i -e "s/NAMESERVICES/${nameservices}/g" ${post_hdfssitexml}

        Hamster_find_conffile "Hadoop" ${HADOOP_CONF_FILES:-""} "hdfs-site-hdfs-federation-namenode.xml" "pre_hdfssitexml_namenodeinfo"

        # Now fill each additional namenode
        for i in `seq 1 ${HDFS_FEDERATION_NAMENODE_COUNT}`
        do
            sed -i -e "/@NAMENODEINFO@/{r ${pre_hdfssitexml_namenodeinfo}" -e "d}" ${post_hdfssitexml}

            sed -i -e "s/NAMESERVICEID/ns${i}/g" ${post_hdfssitexml}

            # First namenode is based on master
            if [ "${i}" == "1" ]
            then
                federationnamenodehost="${HADOOP_MASTER_NODE}"
            else
                numline=`expr ${i} - 1`
                federationnamenodehost=`sed -n "${numline}p" ${HADOOP_CONF_DIR}/namenode_hdfs_federation`
            fi
            
            sed -i -e "s/FEDERATIONNAMENODEHOST/${federationnamenodehost}/g" ${post_hdfssitexml}
        done

        sed -i -e "/@NAMENODEINFO@/,+1d" ${post_hdfssitexml}
    fi

    HADOOP_THIS_HOST=$(Hamster_hostname)

    sed -i \
        -e "s/HADOOP_MASTER_HOST/${HADOOP_MASTER_NODE}/g" \
        -e "s/HADOOP_HOST/${HADOOP_THIS_HOST}/g" \
        -e "s/HDFSBLOCKSIZE/${hdfsblocksize}/g" \
        -e "s/HDFSREPLICATION/${hdfsreplication}/g" \
        -e "s/HDFSNAMENODEHANDLERCLOUNT/${namenodehandlercount}/g" \
        -e "s/HDFSDATANODEHANDLERCLOUNT/${datanodehandlercount}/g" \
        -e "s/IOBUFFERSIZE/${iobuffersize}/g" \
        -e "s/HADOOPHOSTSINCLUDEFILENAME/${hostsincludefilesubst}/g" \
        -e "s/HADOOPHOSTSEXCLUDEFILENAME/${hostsexcludefilesubst}/g" \
        -e "s/DFSDATANODEDIR/${datanodedir}/g" \
        -e "s/DFSNAMENODERPCADDRESS/${default_hadoop_hdfs_namenode_address}/g" \
        -e "s/DFSNAMENODESECONDARYHTTPADDRESS/${default_hadoop_hdfs_namenode_secondary_http_address}/g" \
        -e "s/DFSNAMENODESECONDARYHTTPSADDRESS/${default_hadoop_hdfs_namenode_secondary_https_address}/g" \
        -e "s/DFSNAMENODEHTTPADDRESS/${default_hadoop_hdfs_namenode_httpaddress}/g" \
        -e "s/DFSDATANODEADDRESS/${default_hadoop_hdfs_datanode_address}/g" \
        -e "s/DFSDATANODEHTTPADDRESS/${default_hadoop_hdfs_datanode_httpaddress}/g" \
        -e "s/DFSDATANODEIPCADDRESS/${default_hadoop_hdfs_datanode_ipcaddress}/g" \
        -e "s/DFSBACKUPADDRESS/${default_hadoop_hdfs_namenode_backup_address}/g" \
        -e "s/DFSBACKUPHTTPADDRESS/${default_hadoop_hdfs_namenode_backup_http_address}/g" \
        -e "s/DFSPERMISSIONSSUPERUSERGROUP/${USER}/g" \
        ${post_hdfssitexml}
fi

exit 0
